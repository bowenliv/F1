{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import plot_confusion_matrix as matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "p = pd.read_csv(\"CelticsTrain.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7779/3144214038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#check column with \\xa0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "#check column with \\xa0\n",
    "p.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all occurrences of \\xa0 in data\n",
    "p = p.replace(\"\\xa0\",\" \")\n",
    "#replace all occurrences of \\xa0 in column names\n",
    "p.columns = p.columns.str.replace(\"\\xa0\", \" \")\n",
    "#confirm \\xa0 name is replaced with spaces \n",
    "p.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.groupby(by=['GAME DATE']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = p['W/L']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = [\"MIN\",\"PTS\",\"FGM\",\"FGA\",\"FG%\",\"3PM\",\"3PA\",\"3P%\",\"FTM\",\"FTA\",\"FT%\",\"OREB\",\"DREB\",\"REB\",\"AST\",\"STL\",\"BLK\",\"TOV\", \"PF\",\"+/-\"]\n",
    "features = [\"PTS\",\"FGM\",\"FGA\",\"FG%\",\"3PM\",\"3PA\",\"3P%\",\"FTM\",\"FTA\",\"FT%\",\"OREB\",\"DREB\",\"REB\",\"AST\",\"STL\",\"BLK\",\"TOV\", \"PF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe of just features\n",
    "X = p.loc[:,features]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup plot\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.rcParams.get('figure.figsize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup figure size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 20\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = p[\"W/L\"]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#set how many folds/combinations you want\n",
    "k=5\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "#X is all features and Y is all labels, cv performs the split\n",
    "scores = cross_val_score(clf, X, Y, cv=k)\n",
    "#clf = clf.fit(X, Y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the mean accuracy, and the margin of error at 95% confidence interval. I.e., we are 95% confident that the mean\n",
    "#accuracy is 68% +- 2 standard deviations/sqrt(k) (the more deviations the more confident we can be, widening the beam/net).\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), (scores.std() * 2)/k**(1/2)))\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sorted labels for plot \n",
    "import numpy as np\n",
    "sorted = labels.unique()\n",
    "sorted = np.sort(sorted)\n",
    "sorted = list(map(str, sorted))\n",
    "sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "#scatter_matrix(p, alpha=0.2,figsize=(20, 6))\n",
    "#scatter_matrix(p, alpha=0.2,figsize=(200,500 ), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = tree.plot_tree(clf,feature_names=features,class_names=labels.astype(str),rounded=True,filled=True) \n",
    "#now you have to fit k models using the various splits\n",
    "#after showing that there is isn't much sampling skew you can cross validate this way and choose a tree closest to the \n",
    "#mean value above or take a worst case\n",
    "#note the splitting of folds occurs by randomly, use random state for reproducible results\n",
    "#define a dataframe to store all the models\n",
    "models = pd.DataFrame(columns=[\"score\",\"model\"])\n",
    "\n",
    "#loop over all possible models\n",
    "from sklearn.model_selection import KFold\n",
    "#shuffle reorders the data for less bias\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = 0)\n",
    "i=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train = X.iloc[train_index]\n",
    "    x_test = X.iloc[test_index]\n",
    "    y_train = Y.iloc[train_index]\n",
    "    y_test= Y.iloc[test_index]\n",
    "    \n",
    "    #train the model\n",
    "    model = clf.fit(x_train,y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    print(\"model score = \",model.score(x_test,y_test))\n",
    "    \n",
    "    #print('Scores from each Iteration: ', scores)\n",
    "    print('Average K-Fold Score :' , np.mean(scores)) \n",
    "    plt.figure()\n",
    "    x = tree.plot_tree(model,rounded=True,filled=True,class_names=sorted,feature_names=features) \n",
    "    #assign model to dataframe\n",
    "    models.loc[i] = [model.score(x_test,y_test),model]\n",
    "    #clf = model\n",
    "    #increment index in dataframe\n",
    "    i = i+1\n",
    "    \n",
    "    #plot the confusion matrices 1 for normalzied the other un-normalized\n",
    "    values = ['true',None]\n",
    "    #cmap='cividis'\n",
    "    for x in values:\n",
    "        disp = matrix(clf,X,Y,display_labels=sorted,normalize=x)\n",
    "        disp.ax_.set_title(\"Confusion matrix with normalization = \"+str(x))\n",
    "    print(disp.confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display computed models\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the minimum model with least score\n",
    "lowVal = models.score.min()\n",
    "print('lowVal = ',lowVal)\n",
    "#assign our model to be the least\n",
    "least = models.query(\"score == \"+str(lowVal))\n",
    "clf = least['model'].values[0]\n",
    "clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create a new model with all the data as the running model but remember to use your cross val score\n",
    "#as your accuracy\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf = clf.fit(X,Y)\n",
    "finalPredictions = clf.predict(X)\n",
    "#print(finalPredictions)\n",
    "#print(Y)\n",
    "finalError = clf.score(X,finalPredictions)\n",
    "print(f\"final error/accuracy run on all data = {finalError}\")\n",
    "highVal = models.score.max()\n",
    "meanVal = models.score.mean()\n",
    "sdev = models.score.std()\n",
    "print(f\"cross validation metrics, worst case accuracy = {lowVal}, average = {meanVal}, best case = {highVal}, all +-{2*sdev/k**(1/2)} @ 95%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup plots for confusion matrix\n",
    "figSize = plt.rcParams[\"figure.figsize\"]\n",
    "figSize[0] = 30\n",
    "figSize[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"]=figSize\n",
    "print(plt.rcParams.get('figure.figsize'))\n",
    "\n",
    "#plot the confusion matrices 1 for normalzied the other un-normalized\n",
    "values = ['true',None]\n",
    "#cmap='cividis'\n",
    "for x in values:\n",
    "    disp = matrix(clf,X,Y,display_labels=sorted,normalize=x)\n",
    "    disp.ax_.set_title(\"Confusion matrix with normalization = \"+str(x))\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get false positives\n",
    "pd.set_option('display.max_rows',100)\n",
    "p[(Y!=finalPredictions)&(finalPredictions==\"W\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tree.plot_tree(clf,rounded=True,filled=True,class_names=sorted,feature_names=features) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get false negative\n",
    "pd.set_option('display.max_rows',100)\n",
    "p[(Y!=finalPredictions)&(finalPredictions==\"L\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can run the same analysis on just the test set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"../src/test/resources/CelticsTest.csv\")\n",
    "#testData=test.toPandas()\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get X features\n",
    "XTest = testData.loc[:,features]\n",
    "XTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest = testData[\"W/L\"]\n",
    "YTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on test data \n",
    "YPredicted = clf.predict(XTest)\n",
    "YPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy\n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(YTest,YPredicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup plots for confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix as matrix\n",
    "figSize = plt.rcParams[\"figure.figsize\"]\n",
    "figSize[0] = 30\n",
    "figSize[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"]=figSize\n",
    "print(plt.rcParams.get('figure.figsize'))\n",
    "\n",
    "#plot the confusion matrices 1 for normalzied the other un-normalized\n",
    "values = ['true',None]\n",
    "#cmap='cividis'\n",
    "for x in values:\n",
    "    disp = matrix(clf,XTest,YTest,display_labels=sorted,normalize=x)\n",
    "    disp.ax_.set_title(\"Confusion matrix with normalization = \"+str(x))\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get false positives\n",
    "pd.set_option('display.max_rows',100)\n",
    "testData[(YTest!=YPredicted)&(YPredicted==\"W\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tree.plot_tree(clf,rounded=True,filled=True,class_names=sorted,feature_names=features) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get false negative\n",
    "pd.set_option('display.max_rows',100)\n",
    "testData[(YTest!=YPredicted)&(YPredicted==\"L\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
